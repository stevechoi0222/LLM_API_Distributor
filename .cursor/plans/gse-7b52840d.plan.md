<!-- 7b52840d-2334-44ad-be34-24287ac8f02b 1474c35f-3d86-4f32-b171-1133f31ef02a -->
# GSE Marketing/Visibility Engine Implementation Plan

## Overview

Build a production-ready FastAPI application that executes questions against GSE provider APIs (OpenAI GPT-5, Gemini, Perplexity), tracks responses/citations/metrics, and exports results. The system must work with an externally-managed PostgreSQL database (no superuser rights), use Redis for all locking/rate-limiting, and support both full schema and compat mode deployment.

## Key Constraints & Design Decisions

**Database**:

- External managed Postgres (prod/staging) - RW role only, no superuser
- All tables under `geo_app` schema (configurable)
- Migrations: local auto-apply, prod/staging offline SQL generation for DBA handoff
- No DB locks - Redis handles all concurrency/idempotency
- No extensions required (app-side UUID generation)
- JSONB optional with TEXT JSON fallback
- Compat mode: 2-table minimal schema (`events`, `results`) for restricted environments

**Stack**:

- FastAPI (async), Python 3.11+
- SQLAlchemy async engine with pgbouncer support
- Celery + Redis workers
- Alembic migrations (schema-aware, offline SQL generation)
- Pydantic v2 schemas
- Docker Compose (dev: all services; prod: no postgres)

**Auth**: API Key based

**Project Structure**: `backend/` subdirectory (preserve existing static site)

## Implementation Phases

### Phase 1: Project Scaffold & Infrastructure (M1)

**File Structure**:

```
backend/
  app/
    main.py
    api/
      v1/
        routes_campaigns.py
        routes_ingest.py
        routes_runs.py
        routes_exports.py
      deps.py          # Dependencies (auth, db session)
    core/
      config.py        # Pydantic Settings
      logging.py
      rate_limit.py    # Redis token bucket
      idempotency.py   # Redis-based idempotency
      security.py      # API key validation
    db/
      base.py
      models.py        # SQLAlchemy models
      compat.py        # Compat mode repository
      session.py       # Async engine + session factory
      migrations/      # Alembic
        env.py         # Schema-aware
        versions/
    domain/
      schemas.py       # Pydantic models
      providers/
        base.py
        openai_client.py
        gemini_client.py (stub)
        perplexity_client.py (stub)
      services/
        run_service.py
        ingest_service.py
        export_service.py
        prompt_service.py
      prompts/
        templates/
          default_questions.yaml  # Template-based Q generation
    workers/
      celery_app.py
      tasks.py         # execute_run_item, export_job
    exporters/
      base.py
      csv_exporter.py
      xlsx_exporter.py
      jsonl_exporter.py
      mappers/
        base.py
        example_webhook.py
    utils/
      excel.py
      hashing.py
  scripts/
    migrations_offline.sh
  artefacts/
    migrations.sql     # Generated offline SQL
  tests/
    conftest.py
    test_api/
    test_services/
    test_providers/
  Dockerfile
  docker-compose.yml
  docker-compose.override.yml  # Dev overrides
  alembic.ini
  pyproject.toml
  .env.example
  README.md
docs/
  DB_HANDOFF.md
```

**Configuration** (`backend/app/core/config.py`):

- Pydantic Settings with env vars:
  - `DATABASE_URL`, `LOCAL_DATABASE_URL`
  - `DB_SCHEMA` (default: `geo_app`)
  - `DB_APPLY_MIGRATIONS` (default: false)
  - `DB_COMPAT_MODE` (default: false)
  - `USE_JSONB` (default: true)
  - `REDIS_URL`
  - `CELERY_BROKER_URL`, `CELERY_RESULT_BACKEND`
  - `API_KEYS` (comma-separated)
  - `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `PERPLEXITY_API_KEY`

**Docker Compose**:

- `docker-compose.yml`: api, worker, redis, postgres (dev only)
- `docker-compose.override.yml`: local DB overrides, auto-migrations for dev
- Prod deployment: remove postgres service, point to external DB

**Logging**: Structured logging with request IDs, run IDs

### Phase 2: Database Layer & Migrations (M2)

**SQLAlchemy Setup** (`backend/app/db/session.py`):

- Async engine with connection pooling:
  - `pool_size=10`, `max_overflow=20`, `pool_recycle=1800`, `pool_pre_ping=True`
  - Support `StaticPool` for pgbouncer
- On connect: `SET search_path TO {DB_SCHEMA}, public`
- Startup logic:
  - If `DB_APPLY_MIGRATIONS=true`: run `alembic upgrade head`
  - Else: log "external DB mode, migrations managed by DBA"

**Models** (`backend/app/db/models.py`):

Under `geo_app` schema:

- `campaigns(id, name, product_name, created_at)`
- `topics(id, campaign_id→campaigns, title, description)`
- `personas(id, name, role, domain, locale, tone, extra_json)`
- `questions(id, topic_id→topics, persona_id→personas, text, metadata_json)`
- `runs(id, campaign_id, label, provider_settings_json, status, created_at, started_at, finished_at)`
- `run_items(id, run_id→runs, question_id→questions, idempotency_key UNIQUE, status, attempt_count, last_error, created_at, updated_at)`
- `responses(id, run_item_id→run_items, provider, model, prompt_version, request_json, response_json, text, citations_json, token_usage_json, latency_ms, created_at)`
- `exports(id, run_id→runs, format, mapper_name, config_json, status, file_url, created_at)`
- `files(id, type, url_or_path, parsed_summary_json, uploaded_at)`

JSON columns: JSONB if `USE_JSONB=true`, else TEXT with app-side JSON serialization

**Compat Mode** (`backend/app/db/compat.py`):

- Tables: `geo_app.events`, `geo_app.results`
- Repository pattern to write normalized records as JSON
- Exporters read from compat tables and reconstruct data

**Alembic** (`backend/app/db/migrations/env.py`):

- Read `DB_SCHEMA` from config
- Set `schema={DB_SCHEMA}` on `MetaData`
- Generate idempotent SQL:
  - `CREATE SCHEMA IF NOT EXISTS geo_app` (local only)
  - `CREATE TABLE IF NOT EXISTS`
  - `ALTER TABLE ... ADD COLUMN IF NOT EXISTS` (Postgres 9.6+)
- Offline migration script (`scripts/migrations_offline.sh`):
  ```bash
  #!/bin/bash
  mkdir -p artefacts
  DB_SCHEMA=${DB_SCHEMA:-geo_app} \
  alembic upgrade head --sql > artefacts/migrations.sql
  ```


### Phase 3: Core Services & Idempotency (M3)

**Idempotency** (`backend/app/core/idempotency.py`):

- Redis-based key storage: `idempotency:{hash}` → run_item_id
- Hash function: `hash(provider, model, prompt_version, question_id, persona_id, normalized_question_text, provider_settings)`
- Check before enqueuing task; return existing run_item if duplicate

**Rate Limiting** (`backend/app/core/rate_limit.py`):

- Redis token bucket per provider
- Configurable QPS and burst limits
- Block/wait logic in Celery tasks

**Security** (`backend/app/core/security.py`):

- API key validation middleware
- Read keys from `API_KEYS` env (comma-separated)

**Pydantic Schemas** (`backend/app/domain/schemas.py`):

- Request/response models for all endpoints
- Validation for provider settings, question formats, export configs

### Phase 4: Provider Clients (M4)

**Base Provider** (`backend/app/domain/providers/base.py`):

```python
class ProviderClient(ABC):
    @abstractmethod
    async def prepare_prompt(self, question, persona, topic, prompt_version) -> str:
        pass
    
    @abstractmethod
    async def invoke(self, request, **settings) -> ProviderResult:
        pass

@dataclass
class ProviderResult:
    text: str
    citations: list[str]
    usage: dict  # prompt_tokens, completion_tokens
    latency_ms: int
    raw_response: dict
```

**OpenAI Client** (`backend/app/domain/providers/openai_client.py`):

- Async HTTP client with retries, exponential backoff, circuit breaker
- Timeout configuration
- GPT-5 family placeholder (use `gpt-4` or configurable model)
- Parse citations from response if available

**Stubs**: Gemini, Perplexity clients with NotImplementedError

### Phase 5: Ingestion & Question Generation (M5)

**Excel Ingestion** (`backend/app/domain/services/ingest_service.py`):

- Parse XLSX/CSV with `pandas` or `openpyxl`
- Mapping profile (store in DB or config):
  - Fuzzy column matching: `topic_title`, `persona_name`, `question_text`, etc.
- Validate and preview parsed data
- Store raw file metadata in `files` table
- Write canonical records to `questions`, `personas`, `topics`
- Compat mode: write to `events` table

**Question Import from Agent** (`backend/app/domain/services/ingest_service.py`) - TICKET 1:

- Accept JSONL or JSON array with campaign, topic, persona, question payload
- Upsert campaign, topic, persona (idempotent)
- Insert question records with uniqueness per campaign+topic
- Return import summary: `{"imported": N, "skipped": M, "errors": [...]}`
- Handle large imports (≥140 items) in <2s parsing
- Template-based generation: stub for future (existing agent provides Q×persona sets)

**API Endpoint** (`backend/app/api/v1/routes_ingest.py`):

- `POST /api/v1/questions:ingest` - file upload (Excel/CSV)
- `POST /api/v1/question-sets:import` - direct import from agent (TICKET 1)
- Optional: `POST /api/v1/question-sets:generate` - template stub for future

### Phase 6: Run Orchestration & Workers (M6)

**Run Service** (`backend/app/domain/services/run_service.py`):

- Create run with provider settings, sampling params
- Materialize `run_items` from questions × providers
- Enqueue Celery tasks with rate-limiting

**Celery Worker** (`backend/app/workers/celery_app.py`, `tasks.py`):

- Task: `execute_run_item(run_item_id)`
  - Check Redis idempotency
  - Apply rate limit (token bucket)
  - Invoke provider client
  - Store response in DB (or compat table)
  - Update run_item status
  - Retry on failure with backoff
- Task: `export_job(export_id)` - generate export file

**API Endpoints** (`backend/app/api/v1/routes_runs.py`):

- `POST /api/v1/runs` - create run
- `POST /api/v1/runs/{id}/start` - enqueue tasks
- `GET /api/v1/runs/{id}` - status summary (counts, ETA, errors)
- `GET /api/v1/runs/{id}/items` - paginated results with filters
- `POST /api/v1/runs/{id}/resume` - re-enqueue failed items

### Phase 7: Exports & Mappers (M7)

**Exporters** (`backend/app/exporters/`):

- CSV: `csv_exporter.py` - pandas DataFrame → CSV
- XLSX: `xlsx_exporter.py` - openpyxl or pandas → Excel
- JSONL: `jsonl_exporter.py` - line-delimited JSON

**Mappers** (`backend/app/exporters/mappers/`):

- Base mapper interface
- Example webhook mapper: transform normalized schema → partner API payload, POST to webhook URL

**API Endpoint** (`backend/app/api/v1/routes_exports.py`):

- `GET /api/v1/runs/{id}/results:download?format=csv|xlsx|jsonl`
- `POST /api/v1/exports` - create export job (async via Celery)

### Phase 8: Health, Observability & Testing (M8)

**Health Endpoint**:

- `GET /healthz` - check Redis, DB (`SELECT 1`), return 200 or 503

**Testing** (`backend/tests/`):

- Pytest with async fixtures
- Local Postgres in Docker for integration tests
- Mock provider HTTP calls
- Test compat mode separately
- CI job: generate offline SQL and validate syntax

**Documentation** (`docs/DB_HANDOFF.md`):

- How to generate migration SQL
- Required DB privileges (RW role, schema access)
- Normal vs compat mode deployment
- DBA handoff process

## Environment Variables

`.env.example`:

```
# Database
DATABASE_URL=postgresql+psycopg://user:pass@host:port/dbname
LOCAL_DATABASE_URL=postgresql+psycopg://geo:geo@postgres:5432/geo
DB_SCHEMA=geo_app
DB_APPLY_MIGRATIONS=false
DB_COMPAT_MODE=false
USE_JSONB=true

# Redis
REDIS_URL=redis://redis:6379/0

# Celery
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# Security
API_KEYS=dev-key-123,prod-key-456

# Provider APIs
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=
PERPLEXITY_API_KEY=

# Logging
LOG_LEVEL=INFO
```

## Acceptance Criteria

✓ App boots with external DB using RW role (no superuser)

✓ `scripts/migrations_offline.sh` produces `artefacts/migrations.sql` with idempotent DDL

✓ Compat mode (`DB_COMPAT_MODE=true`) runs without creating new tables

✓ Redis handles all idempotency, rate-limiting, locks (no DB locks)

✓ Can ingest Excel/CSV with flexible column mapping

✓ Can execute runs against OpenAI API with retries and rate-limiting

✓ Can export results in CSV/XLSX/JSONL

✓ Health endpoint checks DB and Redis

✓ Docker Compose runs all services locally with auto-migrations

✓ Comprehensive tests pass

## Files to Create/Modify

**Core files** (40+):

- Docker: `backend/Dockerfile`, `backend/docker-compose.yml`, `backend/docker-compose.override.yml`
- Config: `backend/pyproject.toml`, `backend/alembic.ini`, `backend/.env.example`
- App: `backend/app/main.py`, `backend/app/core/config.py`, `backend/app/db/session.py`, `backend/app/db/models.py`, `backend/app/db/compat.py`
- Migrations: `backend/app/db/migrations/env.py`, initial migration
- Providers: 4 provider files (base + 3 implementations)
- Services: 4 service files (ingest, run, export, prompt)
- API: 4 route files + deps
- Workers: 2 Celery files
- Exporters: 4 exporter files + 2 mappers
- Utils: 3 utility files
- Tests: 10+ test files
- Scripts: `backend/scripts/migrations_offline.sh`
- Docs: `docs/DB_HANDOFF.md`, `backend/README.md`

**Estimated**: ~50 new files, 5000+ LOC

### To-dos

- [ ] Create backend/ directory structure, pyproject.toml with dependencies (fastapi, sqlalchemy[asyncio], alembic, celery, redis, pydantic, etc.), .env.example, and basic README
- [ ] Create Dockerfile, docker-compose.yml (api, worker, redis, postgres), and docker-compose.override.yml for local dev with auto-migrations
- [ ] Implement core/config.py (Pydantic Settings with all env vars), core/logging.py (structured logging), and core/security.py (API key validation)
- [ ] Implement db/session.py with async SQLAlchemy engine, connection pooling, search_path setting, and startup migration logic
- [ ] Create db/models.py with all SQLAlchemy models (campaigns, topics, personas, questions, runs, run_items, responses, exports, files) under geo_app schema with JSONB/TEXT toggle
- [ ] Configure alembic.ini, implement schema-aware db/migrations/env.py, create initial migration, and scripts/migrations_offline.sh
- [ ] Implement db/compat.py with repository pattern for 2-table minimal schema (events, results)
- [ ] Create domain/schemas.py with all Pydantic request/response models for API contracts
- [ ] Implement core/idempotency.py (Redis-based hash storage) and core/rate_limit.py (Redis token bucket per provider)
- [ ] Create domain/providers/base.py with ProviderClient interface and ProviderResult dataclass
- [ ] Implement domain/providers/openai_client.py with async HTTP, retries, circuit breaker, timeout, and response parsing
- [ ] Create domain/providers/gemini_client.py and perplexity_client.py as stubs with NotImplementedError
- [ ] Implement domain/services/ingest_service.py with Excel/CSV parsing, column mapping, validation, and DB writes (normal + compat mode)
- [ ] Implement domain/services/prompt_service.py with template-based question generation from YAML, create domain/prompts/templates/default_questions.yaml
- [ ] Implement domain/services/run_service.py for creating runs, materializing run_items, and orchestration logic
- [ ] Create workers/celery_app.py (Celery config) and workers/tasks.py (execute_run_item, export_job tasks with retry logic)
- [ ] Implement exporters/csv_exporter.py, xlsx_exporter.py, jsonl_exporter.py, and domain/services/export_service.py
- [ ] Create exporters/mappers/base.py and example_webhook.py with partner API transformation and HTTP POST
- [ ] Implement api/deps.py with dependency injection for DB session, API key validation, etc.
- [ ] Create api/v1/routes_ingest.py with POST /questions:ingest (file upload) and POST /question-sets:generate endpoints
- [ ] Create api/v1/routes_runs.py with POST /runs, POST /runs/{id}/start, GET /runs/{id}, GET /runs/{id}/items, POST /runs/{id}/resume
- [ ] Create api/v1/routes_exports.py with GET /runs/{id}/results:download and POST /exports endpoints
- [ ] Create api/v1/routes_campaigns.py with POST /campaigns, POST /campaigns/{id}/topics, POST /personas (basic CRUD)
- [ ] Implement app/main.py with FastAPI app, router registration, CORS, health endpoint (/healthz), startup/shutdown events
- [ ] Create utils/excel.py (Excel helpers), utils/hashing.py (idempotency hash), utils/time.py (timestamp utilities)
- [ ] Create tests/conftest.py with async fixtures, DB setup, mock providers, and test utilities
- [ ] Write integration tests for API endpoints, run lifecycle, ingestion, and exports
- [ ] Create docs/DB_HANDOFF.md with migration generation instructions, privilege requirements, and deployment modes